{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed50ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7e74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jiwer in /home/nmu1/.local/lib/python3.10/site-packages (3.0.4)\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /home/nmu1/.local/lib/python3.10/site-packages (from jiwer) (3.9.3)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /home/nmu1/.local/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71306379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers==4.31.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4da09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets==2.14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f81d681",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00913bc0-ddc6-466e-8b3e-48480b35d076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c978578-4b12-4bfa-ab48-4415ab5dce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453f5b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5746166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/nmu1/.local/lib/python3.10/site-packages (4.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/nmu1/.local/lib/python3.10/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nmu1/.local/lib/python3.10/site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/nmu1/.local/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/nmu1/.local/lib/python3.10/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/nmu1/.local/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/nmu1/.local/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nmu1/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nmu1/.local/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/nmu1/.local/lib/python3.10/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22842871-3ee5-4cbe-a1e6-bf30d0564f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from datasets import load_metric\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debcdb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code = 'ar'\n",
    "language_name = 'arabic'\n",
    "\n",
    "#base_model = \"facebook/wav2vec2-xls-r-300m\"\n",
    "base_model = \"/home/nmu1/Downloads/MTC/Train Model/40-1-1/output_models_Demo40_1-1/ar/facebookwav2vec2-xls-r-300m/finetuned-v4\"\n",
    "output_models_dir = f\"/home/nmu1/Downloads/MTC/Train Model/40-2-2/output_models_Demo_random40-2-2/ar/facebookwav2vec2-xls-r-300m/finetuned-v17\"\n",
    "new_output_models_dir = f\"/home/nmu1/Downloads/MTC/Train Model/40-2-2/output_models_Demo_random40-2-2/ar/facebookwav2vec2-xls-r-300m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(f\"/home/nmu1/Downloads/MTC/Train Model/40-2-2/train_V_random-40-2-2.pbz2\", compression='bz2')\n",
    "dev = pd.read_pickle(f\"/home/nmu1/Downloads/MTC/Train Model/40-2-2/adapt_V_random-40-2-2.pbz2\", compression='bz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643c686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_values', 'input_length', 'labels']\n"
     ]
    }
   ],
   "source": [
    "print(train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d081577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        input_values  input_length  \\\n",
      "0  [0.06711511, 0.011185066, -0.012870867, -0.062...        131360   \n",
      "1  [-0.00038258886, -0.00038258886, -0.0003825888...         99680   \n",
      "2  [-0.00028004573, -0.00028004573, -0.0002800457...        116960   \n",
      "3  [0.000388785, 0.000388785, 0.000388785, 0.0003...         96800   \n",
      "4  [5.477058e-05, 5.477058e-05, 5.477058e-05, 5.4...        120320   \n",
      "\n",
      "                                              labels  \n",
      "0  [5, 2, 28, 7, 35, 28, 31, 10, 12, 34, 16, 34, ...  \n",
      "1  [4, 7, 35, 28, 35, 12, 35, 31, 16, 34, 16, 28,...  \n",
      "2  [20, 35, 31, 30, 3, 36, 28, 35, 16, 4, 28, 35,...  \n",
      "3  [19, 35, 34, 14, 4, 28, 35, 19, 31, 13, 36, 28...  \n",
      "4  [35, 12, 36, 34, 6, 28, 20, 34, 28, 14, 34, 35...  \n",
      "                                        input_values  input_length  \\\n",
      "0  [-0.0010540755, -0.0010540755, -0.0010540755, ...         26112   \n",
      "1  [0.0010558531, 0.0010558531, 0.0010558531, 0.0...         72192   \n",
      "2  [-0.0016085549, -0.0016085549, -0.0016085549, ...         91533   \n",
      "3  [-0.00037587585, -0.00037587585, -0.0003758758...         42240   \n",
      "4  [1.9463145e-05, 1.9463145e-05, 1.9463145e-05, ...         54144   \n",
      "\n",
      "                                              labels  \n",
      "0  [25, 3, 20, 12, 16, 35, 28, 35, 12, 7, 25, 3, ...  \n",
      "1  [12, 35, 28, 12, 12, 35, 6, 20, 28, 17, 32, 31...  \n",
      "2  [3, 35, 12, 36, 35, 24, 12, 28, 31, 2, 30, 12,...  \n",
      "3  [3, 28, 35, 16, 31, 28, 32, 34, 20, 28, 19, 36...  \n",
      "4  [7, 34, 19, 36, 20, 3, 25, 28, 1, 35, 24, 4, 2...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train = train.to_pandas()\n",
    "print(train.head())\n",
    "dev = dev.to_pandas()\n",
    "print(dev.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84502661",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(new_output_models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b71da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MTCDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.data = df[[\"input_values\", \"input_length\", \"labels\"]].to_dict('records')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c44db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCollatorCTCWithPadding:\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "        \n",
    "    def __init__(self, processor: Wav2Vec2Processor, padding: Union[bool, str]):\n",
    "        self.processor = processor\n",
    "        self.padding = padding\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        with self.processor.as_target_processor():\n",
    "            labels_batch = self.processor.pad(\n",
    "                label_features,\n",
    "                padding=self.padding,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25976b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fdb370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6039/24688115.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "wer_metric = load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3dbf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2620e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at /home/nmu1/Downloads/MTC/Train Model/40-1-1/output_models_Demo40_1-1/ar/wav2vec2-large-xlsr-arabic/finetuned-v4 and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([38, 1024]) in the checkpoint and torch.Size([41, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([38]) in the checkpoint and torch.Size([41]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    base_model, \n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c3c695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at /home/nmu1/Downloads/MTC/Train Model/40-1-1/output_models_Demo40_1-1/ar/wav2vec2-large-xlsr-arabic/finetuned-v4 and are newly initialized because the shapes did not match:\n",
      "- lm_head.weight: found shape torch.Size([38, 1024]) in the checkpoint and torch.Size([41, 1024]) in the model instantiated\n",
      "- lm_head.bias: found shape torch.Size([38]) in the checkpoint and torch.Size([41]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTC.from_pretrained(\n",
    "    base_model, \n",
    "    attention_dropout=0.0,\n",
    "    hidden_dropout=0.0,\n",
    "    feat_proj_dropout=0.0,\n",
    "    mask_time_prob=0.05,\n",
    "    layerdrop=0.0,\n",
    "    ctc_loss_reduction=\"mean\", \n",
    "    pad_token_id=processor.tokenizer.pad_token_id,\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05b22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_feature_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09190d-6466-46c9-8974-e4eb04209928",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "  output_dir=new_output_models_dir,\n",
    "  group_by_length=True,\n",
    "  per_device_train_batch_size=16,\n",
    "  gradient_accumulation_steps=2,\n",
    "  evaluation_strategy=\"steps\",\n",
    "  num_train_epochs=10,\n",
    "  gradient_checkpointing=True,\n",
    "  fp16=True,\n",
    "  save_steps=600,\n",
    "  eval_steps=300,\n",
    "  logging_steps=300,\n",
    "  learning_rate=3e-4,\n",
    "  warmup_steps=500,\n",
    "  save_total_limit=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6623b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MTCDataset(train)\n",
    "dev_dataset = MTCDataset(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d34f19-9d62-4060-b1eb-cb963f442d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=training_args.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06afc3c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    tokenizer=processor.feature_extractor,\n",
    "    optimizers=(optimizer, None)  # Pass the optimizer here\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e563ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nmu1/Downloads/MTC/Train Model/40-2-2/wandb/run-20240701_233630-h36v7kym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/3bdoai/huggingface/runs/h36v7kym' target=\"_blank\">balmy-grass-68</a></strong> to <a href='https://wandb.ai/3bdoai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/3bdoai/huggingface' target=\"_blank\">https://wandb.ai/3bdoai/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/3bdoai/huggingface/runs/h36v7kym' target=\"_blank\">https://wandb.ai/3bdoai/huggingface/runs/h36v7kym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6340' max='6340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6340/6340 3:15:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Wer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>4.064700</td>\n",
       "      <td>1.052263</td>\n",
       "      <td>0.516978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.268300</td>\n",
       "      <td>1.125242</td>\n",
       "      <td>0.533460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.231300</td>\n",
       "      <td>1.160548</td>\n",
       "      <td>0.526356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.223000</td>\n",
       "      <td>1.095631</td>\n",
       "      <td>0.520152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>1.198022</td>\n",
       "      <td>0.521857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.168800</td>\n",
       "      <td>1.222952</td>\n",
       "      <td>0.520625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.146800</td>\n",
       "      <td>1.258187</td>\n",
       "      <td>0.531092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.143200</td>\n",
       "      <td>1.303570</td>\n",
       "      <td>0.519583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.125300</td>\n",
       "      <td>1.304834</td>\n",
       "      <td>0.519820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>1.256874</td>\n",
       "      <td>0.519110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.115700</td>\n",
       "      <td>1.335695</td>\n",
       "      <td>0.513000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>1.268296</td>\n",
       "      <td>0.511816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.099900</td>\n",
       "      <td>1.276706</td>\n",
       "      <td>0.509022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.090700</td>\n",
       "      <td>1.310720</td>\n",
       "      <td>0.503102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.088700</td>\n",
       "      <td>1.420731</td>\n",
       "      <td>0.515037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.078800</td>\n",
       "      <td>1.367748</td>\n",
       "      <td>0.500592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.077700</td>\n",
       "      <td>1.393725</td>\n",
       "      <td>0.498698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.071800</td>\n",
       "      <td>1.414746</td>\n",
       "      <td>0.498129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.071300</td>\n",
       "      <td>1.405512</td>\n",
       "      <td>0.496661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>1.411770</td>\n",
       "      <td>0.496045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.065100</td>\n",
       "      <td>1.452332</td>\n",
       "      <td>0.497135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/nmu1/.local/lib/python3.10/site-packages/transformers/models/wav2vec2/processing_wav2vec2.py:155: UserWarning: `as_target_processor` is deprecated and will be removed in v5 of Transformers. You can process your labels by using the argument `text` of the regular `__call__` method (either in the same call as your audio inputs, or in a separate call.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6340, training_loss=0.3129705006391844, metrics={'train_runtime': 11723.3518, 'train_samples_per_second': 17.304, 'train_steps_per_second': 0.541, 'total_flos': 4.285596138748804e+19, 'train_loss': 0.3129705006391844, 'epoch': 10.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d602fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(output_models_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
